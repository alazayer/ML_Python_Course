{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coursera - Machine Learning with Python\n",
    "### Week 3 Material\n",
    "\n",
    "Date: March 17, 2020\n",
    "\n",
    "## Classification\n",
    "\n",
    "A supervised learning approach aimed at categorizing a set of data. Types of classification are:\n",
    "\n",
    "1- Decision Trees\n",
    "\n",
    "2- Naive Bayes\n",
    "\n",
    "3- Linear Discriminant Analysis\n",
    "\n",
    "4- k-Nearest Neighbor\n",
    "\n",
    "5- Logistic Regression\n",
    "\n",
    "6- Neural Networks\n",
    "\n",
    "7- Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest neighbor\n",
    "\n",
    "Can be used for both classification and regression. It is important to test different values for K to see the optimum for the model. \n",
    "\n",
    "**Evaluation Metrics**\n",
    "\n",
    "*Legend:* **TP:** True Postive, **FN:** False Negative, $y$: actual labels, $\\hat{y}$: predicted labels\n",
    "\n",
    "- Jaccard Index $\\frac{|y \\bigcup \\hat{y}|}{|y \\bigcap \\hat{y}|}$ \n",
    "- Confusion Matrix: A matrix of true labels vs. predicted labels $\\begin{bmatrix}  TP & FN \\\\ FP & TN \\end{bmatrix}$\n",
    "- Precision: Is the ability of the model to only predict the trues = $\\frac{TP}{TP+FP}$\n",
    "- Recall: Is the ability of the model to identify all the trues. = $\\frac{TP}{TP+FN}$\n",
    "- F1 Score (Ranges from 0 to 1) = $2 \\times \\frac{Prec \\times Rec}{Prec + Rec}$\n",
    "- Log Loss: Is used when the predicted output is a probability between 0 and 1\n",
    "\n",
    "**Jaccard Index**\n",
    "$$\\frac{|y \\bigcup \\hat{y}|}{|y \\bigcap \\hat{y}|}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "The main idea is to assess the significance of the attributes (independent variables) and use that to construct the decision tree where the most significant will be at the top. They're built using \"recursive partitioning\".\n",
    "\n",
    "Recursive partitioning is the processing of splitting the data based on an attribute and seeing the split in results (e.g. we have two drugs (A or B), when we split by gender we get both A and B in each segment which means it is bad). Decision trees use **Impurity** and **Entropy** to measure the quality of the split. \n",
    "\n",
    "If the outcomes in the leaf are exactly split, then entropy is 1 (Not good). If the outcomes are all the same (e.g. Drug A), then entropy is 0 (Good).\n",
    "\n",
    "$$Entropy = -P(A) \\times log{(P(A))} -P(B) \\times log{(P(B))}$$\n",
    "\n",
    "During the recursive partitioning process we calculate entropy before split and the weighted entropy after split. This is used to calculate **Information Gain** (Entropy Before Split - Weighted Entropy After Split)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Similar to linear regression but outputs categorical instead of numeric outputs. It is good when:\n",
    "\n",
    "- Binary output\n",
    "- If we need a probabilistic result\n",
    "- If we can separate the decision with linear boundary\n",
    "- If we need to understand the impact of an independent variable (The value of the weight is an indicator)\n",
    "\n",
    "It uses a Sigmoid function instead of a linear function. The objective is to optimize the weights to reduce the error (cost function) which is the difference between the probabilities calculated and the actual categorical value for each data. (I skipped the part about gradient descent with derivatives...etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "Is a supervised algorithm than can classify cases by finding a separator. It does this by mapping data to a higher-dimensional space (**kernelling**) and then finding a separator.\n",
    "\n",
    "Main question: Is how do we transform to the higher dimensional space and how to determine the separator?\n",
    "\n",
    "We aim for a hyperplane with the biggest margin from the data set (Farthest away from the two sets).\n",
    "\n",
    "Advantages is that they are accurate and is memory efficient\n",
    "Disadvantages is that it is prone to overfitting and it does not give probabilities. Also not very efficient for large data sets.\n",
    "\n",
    "So when should I use SVM? \n",
    "\n",
    "- Imagine analysis\n",
    "- Text mining\n",
    "- Detecting Spam\n",
    "- Sentiment Analysis\n",
    "- Gene expression classification\n",
    "\n",
    "Can also be used in regression, outlier detection and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
